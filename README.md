# Emotion Recognition Using ECG and Voice Signals

This project implements emotion recognition using two physiological and audio-based approaches:  
1. ECG Signal-Based Emotion Detection  
2. Voice-Based Emotion Recognition  

The ECG-based method uses **DEAP dataset features**, while the voice-based method uses speech features for classification.  
Both methods leverage **feature engineering** and machine learning/deep learning models to classify emotions.



Project Overview
The project is divided into two main parts:

1. ECG-Based Emotion Recognition
- Uses ECG signals from the **DEAP dataset**
- Preprocessing to clean raw ECG data
- Feature extraction (statistical and frequency domain features)
- Mapping features to **valence** and **arousal** values
- Classification into emotion categories

2. Voice-Based Emotion Recognition
- Extracts acoustic features from audio recordings
- Uses machine learning/deep learning models for speech emotion classification



Objectives
- Apply data preprocessing and feature extraction techniques
- Implement separate pipelines for ECG and voice emotion recognition
- Compare accuracy and performance of different modalities
- Demonstrate the use of physiological and audio data for emotion recognition



Files in This Repository
